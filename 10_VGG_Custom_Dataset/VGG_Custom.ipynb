{
  "cells": [
    {
      "metadata": {
        "id": "e990e6801202c40d"
      },
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center;\">\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bmalcover/aa_2425/blob/main/10_VGG_Custom_Dataset/VGG_Custom.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "</div>"
      ],
      "id": "e990e6801202c40d"
    },
    {
      "cell_type": "code",
      "id": "272d21b93244ea6",
      "metadata": {
        "id": "272d21b93244ea6"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import glob as glob\n",
        "import cv2"
      ],
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "66a26d98415d24c4"
      },
      "cell_type": "markdown",
      "source": [
        "# Introducció\n",
        "\n",
        "En aquesta pràctica, treballarem amb un conjunt de dades d'imatges de cans i moixos que ja heu utilitzat a la pràctica de la part anterior de l'assignatura. Tot i que coneixem aquest conjunt de dades, en aquesta ocasió ho em d'emprar amb ``Pytorch`` per tant haurem d'adaptar com ho llegim: fent un pre-processat de les dades per aplicar-los nous models d'aprenentatge profund.\n",
        "\n",
        "L'objectiu de la sessió serà experimentar amb les diferents versions de la xarxa de convolució ``VGG``, com ara VGG16 i VGG19. Aquesta pràctica us permetrà veure com canvien els resultats d'entrenament i predicció en funció de l'arquitectura emprada, i reflexionar sobre els avantatges i inconvenients d'incrementar la profunditat d'una xarxa de convolució.\n",
        "\n",
        "# Preparam les dades\n",
        "Primerament descarregam les dades en el Google Colab. Per tal de fer-ho emprarem les eines ``wget`` i ``unzip``.\n"
      ],
      "id": "66a26d98415d24c4"
    },
    {
      "metadata": {
        "id": "94c40c1e9bbd1603"
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/bmalcover/aa_2425/releases/download/v1/gatigos.zip\n",
        "!unzip gatigos.zip"
      ],
      "id": "94c40c1e9bbd1603",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "78dcf25d5b66ce14"
      },
      "cell_type": "markdown",
      "source": [
        "# *Custom dataset*\n",
        "\n",
        "Un dataset personalitzat en PyTorch permet preparar i gestionar dades específiques per a un projecte d’aprenentatge automàtic. La classe ``Dataset`` de ``PyTorch`` és la base per crear aquest tipus de dataset i requereix la implementació de tres mètodes essencials: ``__init__``, ``__len__``, i ``__getitem__``.\n",
        "\n",
        "* ``__init__``: Aquest mètode inicialitza el ``dataset`` i defineix els paràmetres que seran necessaris, com ara la ubicació de les dades o qualsevol transformació a aplicar. Aquí es poden carregar rutes d'imatges o etiquetes i definir les transformacions que es realitzaran.\n",
        "\n",
        "* ``__len__``: Aquest mètode retorna el nombre total d'exemples en el ``dataset``. ``PyTorch`` l'utilitza per saber quantes mostres conté el conjunt de dades, cosa que és essencial per crear les batchs d’entrenament.\n",
        "* ``__getitem__``: Aquest mètode accedeix a una mostra concreta del ``dataset``, identificada per un índex, i retorna les dades i la seva etiqueta (o ``target``). Normalment, s’apliquen les transformacions aquí abans de retornar la mostra, per assegurar que cada dada té el format adequat per al model.\n",
        "\n",
        "Un cop creada, aquesta classe es pot emprar amb el ``DataLoader`` de ``PyTorch`` per gestionar l'entrenament en *batchs*, fent que el dataset personalitzat sigui eficient i fàcil de treballar dins del flux d’aprenentatge automàtic de ``PyTorch``.\n"
      ],
      "id": "78dcf25d5b66ce14"
    },
    {
      "metadata": {
        "id": "c35b3d5385843893"
      },
      "cell_type": "code",
      "source": [
        "class CatIGosDataset(Dataset):\n",
        "    \"\"\"Cat i Gos dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, transform=None):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            transform:\n",
        "        \"\"\"\n",
        "        self.image_paths = glob.glob(\"./**/*.png\")\n",
        "        self.xml_paths = glob.glob(\"./**/*.xml\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = cv2.imread(self.image_paths[idx])\n",
        "        annotation = self.get_dict_xml(idx)\n",
        "        return image, annotation\n",
        "\n",
        "    def get_dict_xml(self, idx):\n",
        "        # Parse XML and extract class name\n",
        "        xml_path = self.xml_paths[idx]\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Extract the class name from <object> -> <name>\n",
        "        class_name = None\n",
        "        for obj in root.findall('object'):\n",
        "            name = obj.find('name')\n",
        "            if name is not None:\n",
        "                class_name = name.text\n",
        "                break  # Get the first class name found\n",
        "\n",
        "        return {\n",
        "            \"path\": xml_path,\n",
        "            \"output\": class_name\n",
        "        }\n"
      ],
      "id": "c35b3d5385843893",
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "class CatIGosDataset(Dataset):\n",
        "    \"\"\"Cat i Gos dataset.\"\"\"\n",
        "\n",
        "    def extract_xml_annotation(self, imagenes, path):\n",
        "      labels={}\n",
        "      for path_img in imagenes:\n",
        "        _,name = os.path.split(path_img)\n",
        "        name = name.split(\".\")[0]\n",
        "\n",
        "        name_xml = path+f\"/annotations/{name}.xml\"\n",
        "        tree = ET.parse(name_xml)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        annotation = root.find('object').find('name').text\n",
        "        annotation = 0 if annotation == 'cat' else 1\n",
        "\n",
        "        labels[path_img] = annotation\n",
        "      return labels\n",
        "\n",
        "    def __init__(self, path, transform=None):\n",
        "      self.imagenes = glob.glob(path+\"/images/*.png\")\n",
        "      self.labels = self.extract_xml_annotation(self.imagenes, path)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imagenes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.imagenes[idx])\n",
        "        label = self.labels[self.imagenes[idx]]\n",
        "        return img, label"
      ],
      "metadata": {
        "id": "Evzj59olWoBY"
      },
      "id": "Evzj59olWoBY",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jorge = CatIGosDataset(\"./\")\n",
        "# jorge = CatIGosDataset()\n",
        "\n",
        "a, b = jorge.__getitem__(7)\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "PGRDxCChPQa3",
        "outputId": "9392bd9a-bedb-42ee-b318-365c044eb0a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PGRDxCChPQa3",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[105  90  94]\n",
            "  [ 99  85  89]\n",
            "  [ 96  82  86]\n",
            "  ...\n",
            "  [111 110 112]\n",
            "  [115 108 111]\n",
            "  [115 104 107]]\n",
            "\n",
            " [[108  93  97]\n",
            "  [105  91  95]\n",
            "  [103  89  93]\n",
            "  ...\n",
            "  [114 111 113]\n",
            "  [123 114 117]\n",
            "  [119 105 109]]\n",
            "\n",
            " [[103  88  92]\n",
            "  [104  90  94]\n",
            "  [104  90  94]\n",
            "  ...\n",
            "  [120 113 116]\n",
            "  [128 117 120]\n",
            "  [120 105 109]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[173 174 172]\n",
            "  [153 154 152]\n",
            "  [162 163 161]\n",
            "  ...\n",
            "  [213 217 205]\n",
            "  [210 214 202]\n",
            "  [203 207 195]]\n",
            "\n",
            " [[172 173 171]\n",
            "  [162 163 161]\n",
            "  [172 173 171]\n",
            "  ...\n",
            "  [201 205 193]\n",
            "  [197 201 189]\n",
            "  [206 210 198]]\n",
            "\n",
            " [[173 174 172]\n",
            "  [168 169 167]\n",
            "  [177 178 176]\n",
            "  ...\n",
            "  [212 216 204]\n",
            "  [207 211 199]\n",
            "  [216 220 208]]]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "f040a135483ff08e",
      "metadata": {
        "id": "f040a135483ff08e",
        "outputId": "45a5d83a-9602-4b24-873f-c93c423861a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "#dataset = CatIGosDataset(\"./\")\n",
        "\n",
        "#train = dataset.ImageFolder('../data/tiny-imagenet/train', transform=transform)\n",
        "#test = dataset.ImageFolder('../data/tiny-imagenet/test', transform=transform)\n",
        "\n",
        "imagenes = glob.glob(\"CatIGos/images/*.png\")\n",
        "\n",
        "train, test = train_test_split(imagenes, test_size=0.34, random_state=42)\n",
        "\n",
        "datasetTrain = CatIGosDataset('./CatIGos',images=train, transform=transform)\n",
        "datasetTest = CatIGosDataset('./CatIGos',images=test, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(datasetTrain,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(datasetTest,\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           shuffle=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.34 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-13425a7f0078>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mimagenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CatIGos/images/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdatasetTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatIGosDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./CatIGos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m                     )\n\u001b[1;32m    212\u001b[0m                 ):\n\u001b[0;32m--> 213\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2786\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2415\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2416\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.34 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "id": "fe645c3e13180bbe",
      "metadata": {
        "id": "fe645c3e13180bbe"
      },
      "source": [
        "# Definicio de la xarxa: VGG i *Transfer learning*\n",
        "\n",
        "En aquesta pràctica aplicarem la tècnica de transfer learning amb una de les xarxes CNN més conegudes i profundes:\n",
        "\n",
        " - VGG. [Very Deep Convolutional Networks for Large-Scale Image Recognition, 2014](https://arxiv.org/abs/1409.1556). La mida d'entrada de les imatges és de (224x224x3). VGG es presenta en diferents variants, com ara VGG16 i VGG19, que contenen respectivament 16 i 19 capes amb aproximadament 138 milions de paràmetres entrenables en el cas de VGG16.\n",
        "\n",
        "Descarregarem VGG i l'analitzarem. En aquest cas, no només obtenim la seva arquitectura, sinó també els pesos resultants del seu entrenament en grans conjunts de dades.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "55fbbcc900043cba",
      "metadata": {
        "id": "55fbbcc900043cba"
      },
      "source": [
        "vgg11 = models.vgg11(weights=True)\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"Arquitectura VGG11\")\n",
        "print(\"-\" * 50)\n",
        "print(vgg11)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e1fbc1cbf5c836a"
      },
      "cell_type": "markdown",
      "source": [
        "## Com emprar la GPU per entrenar un model\n",
        "\n",
        "Un dels elements diferencials d'aquest model, respecte als que havíem vist fins ara, és la seva mida i, per tant, l'entrenament es torna impossible emprant __CPU__ directament. Per resoldre-ho hem d'emprar una **GPU**, a Google Colab disposam d'elles gratuïtament. Per fer-ho amb *Pytorch* hem de fer tres passes:\n",
        "\n",
        "1. Comprovar que hi ha una GPU disponible.\n",
        "2. Moure el model a GPU.\n",
        "3. Moure les dades a GPU.\n",
        "\n",
        "### Comprova si tenim una GPU disponible\n",
        "\n",
        "Primer de tot, cal verificar si hi ha una GPU disponible a l’entorn. Això es pot fer amb el següent codi:\n",
        "\n",
        "```python\n",
        "\n",
        "import torch\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "```\n",
        "\n",
        "Si la variable ``is_cuda`` és certa, llavors tens accés a una GPU.\n",
        "\n",
        "### Mou el model a la GPU\n",
        "\n",
        "En PyTorch, els models han d'estar explícitament en la GPU per poder fer servir la seva potència de càlcul. Si estàs carregant un model preentrenat (com AlexNet, ResNet, etc.), o si has definit el teu propi model, pots moure’l a la GPU amb ``.to(device)``, on device fa referència a la GPU.\n",
        "\n",
        "```python\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "```\n",
        "\n",
        "Això mou el model a la GPU (si està disponible). Si només tens una CPU, el model es mantindrà a la CPU.\n",
        "\n",
        "### Mou les dades a la GPU\n",
        "\n",
        "No només el model, sinó que també les dades (inputs) han d'estar a la GPU per fer les operacions més ràpides. Així, abans de fer servir les dades com a inputs del model, assegura't de moure-les al mateix device:\n",
        "\n",
        "```python\n",
        "\n",
        "# Exemple d'un batch de dades\n",
        "inputs, labels = inputs.to(device), labels.to(device)\n",
        "```\n"
      ],
      "id": "e1fbc1cbf5c836a"
    },
    {
      "metadata": {
        "id": "3cc3f9f3c6064fa8"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "vgg11.to(device)"
      ],
      "id": "3cc3f9f3c6064fa8",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "ada569873f766fc1"
      },
      "cell_type": "markdown",
      "source": [
        "# Entrenament"
      ],
      "id": "ada569873f766fc1"
    },
    {
      "metadata": {
        "id": "135d69bf892ad11b"
      },
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "id": "135d69bf892ad11b",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "5ee807ab8ee2cdf1",
      "metadata": {
        "id": "5ee807ab8ee2cdf1"
      },
      "source": [
        "\n",
        "## Feina a fer:\n",
        "\n",
        "1. Preparar el *dataset* personalitzat.\n",
        "2. Carregar la xarxa VGG11, VGG16 i VGG19, amb i sense batch normalization.\n",
        "3. Entrenar-ho fent *transfer learning*.\n",
        "4. Comparar els resultats.\n"
      ]
    },
    {
      "metadata": {
        "id": "c7697fa2e6da352e"
      },
      "cell_type": "code",
      "source": [],
      "id": "c7697fa2e6da352e",
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}